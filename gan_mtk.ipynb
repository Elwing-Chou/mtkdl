{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan_mtk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YH2rEwrsqFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from keras.datasets.mnist import load_data\n",
        "(x_train, y_train), (x_test, y_test)= load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Tx8YWvnq76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "1baef760-b18b-4054-e728-a032006fb789"
      },
      "source": [
        "# Generator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "generator = Sequential()\n",
        "generator.add(Dense(256, activation=\"relu\", input_dim=100))\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(Dense(512, activation=\"relu\"))\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(Dense(784, activation=\"tanh\"))\n",
        "generator.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 784)               402192    \n",
            "=================================================================\n",
            "Total params: 562,704\n",
            "Trainable params: 561,168\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQJQByzQqozg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "700dce50-8a7e-4b9f-96e9-bae78a05f6df"
      },
      "source": [
        "discriminator = Sequential()\n",
        "discriminator.add(Dense(256, input_dim=784, activation=\"relu\"))\n",
        "discriminator.add(Dropout(0.25))\n",
        "discriminator.add(Dense(1, activation=\"sigmoid\"))\n",
        "discriminator.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 201,217\n",
            "Trainable params: 201,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0UZ2RNAsl65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 訓練流程\n",
        "# 1. 訓練 Discriminator (1: 真的image-MNIST) (0: 假的image-現在的Gen)\n",
        "# 2. 訓練 Generator (接上Dis[不動]在後面當作loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnl0HgbPt8Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 訓練版本(會動)\n",
        "discriminator.compile(loss=\"binary_crossentropy\",\n",
        "                      optimizer=\"adam\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4XZNYUQukOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "701d851d-f190-4eee-8c47-456745c390af"
      },
      "source": [
        "# 訓練gen版本(Dis不動)\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "i = Input(shape=(100, ))\n",
        "gen = generator(i)\n",
        "discriminator.trainable = False\n",
        "out = discriminator(gen)\n",
        "gan = Model(inputs=i, outputs=out)\n",
        "gan.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "sequential_5 (Sequential)    (None, 784)               562704    \n",
            "_________________________________________________________________\n",
            "sequential_10 (Sequential)   (None, 1)                 201217    \n",
            "=================================================================\n",
            "Total params: 763,921\n",
            "Trainable params: 561,168\n",
            "Non-trainable params: 202,753\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P49WiaIwNEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan.compile(loss=\"binary_crossentropy\",\n",
        "            optimizer=\"adam\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeXUrTMlzImX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_reshape = (x_train.reshape(-1, 784) - 127.5) / 127.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eg284I4ydvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c91b0a8d-c4b3-4f52-fd50-64d24ac77b5d"
      },
      "source": [
        "import numpy as np\n",
        "batch = 50\n",
        "\n",
        "for i in range(100):\n",
        "    print(\"Times:\", i)\n",
        "    # 警察看真的\n",
        "    idx = np.random.randint(0, x_reshape.shape[0], batch)\n",
        "    x = x_reshape[idx]\n",
        "    y = np.ones((batch, ))\n",
        "    real_loss = discriminator.train_on_batch(x, y)\n",
        "\n",
        "    # 警察看假的\n",
        "    rand = np.random.normal(0, 1, (batch, 100))\n",
        "    fakex = generator.predict(rand)\n",
        "    fakey = np.zeros((batch, ))\n",
        "    fake_loss = discriminator.train_on_batch(fakex, fakey)\n",
        "\n",
        "    print(\"Dis Loss:\", (real_loss + fake_loss) / 2)\n",
        "\n",
        "    # 小偷\n",
        "    rand = np.random.normal(0, 1, (batch, 100))\n",
        "    geny = np.ones((batch, ))\n",
        "    gen_loss = gan.train_on_batch(rand, geny)\n",
        "    print(\"Gen Loss:\", gen_loss)\n",
        "    print(\"*\" * 30)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Times: 0\n",
            "Dis Loss: 0.6965331435203552\n",
            "Gen Loss: 0.71561813\n",
            "******************************\n",
            "Times: 1\n",
            "Dis Loss: 0.647453248500824\n",
            "Gen Loss: 0.7196187\n",
            "******************************\n",
            "Times: 2\n",
            "Dis Loss: 0.6684439182281494\n",
            "Gen Loss: 0.7331951\n",
            "******************************\n",
            "Times: 3\n",
            "Dis Loss: 0.6646207571029663\n",
            "Gen Loss: 0.73368317\n",
            "******************************\n",
            "Times: 4\n",
            "Dis Loss: 0.6624350547790527\n",
            "Gen Loss: 0.7199841\n",
            "******************************\n",
            "Times: 5\n",
            "Dis Loss: 0.6862897276878357\n",
            "Gen Loss: 0.72240496\n",
            "******************************\n",
            "Times: 6\n",
            "Dis Loss: 0.6374037265777588\n",
            "Gen Loss: 0.77858335\n",
            "******************************\n",
            "Times: 7\n",
            "Dis Loss: 0.5970778465270996\n",
            "Gen Loss: 0.7596234\n",
            "******************************\n",
            "Times: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dis Loss: 0.6352246999740601\n",
            "Gen Loss: 0.7858598\n",
            "******************************\n",
            "Times: 9\n",
            "Dis Loss: 0.6633301973342896\n",
            "Gen Loss: 0.8571744\n",
            "******************************\n",
            "Times: 10\n",
            "Dis Loss: 0.6505597829818726\n",
            "Gen Loss: 0.8125461\n",
            "******************************\n",
            "Times: 11\n",
            "Dis Loss: 0.6387442350387573\n",
            "Gen Loss: 0.78580403\n",
            "******************************\n",
            "Times: 12\n",
            "Dis Loss: 0.6530616879463196\n",
            "Gen Loss: 0.80057204\n",
            "******************************\n",
            "Times: 13\n",
            "Dis Loss: 0.6299878358840942\n",
            "Gen Loss: 0.8502325\n",
            "******************************\n",
            "Times: 14\n",
            "Dis Loss: 0.6549181938171387\n",
            "Gen Loss: 0.7562918\n",
            "******************************\n",
            "Times: 15\n",
            "Dis Loss: 0.6392450332641602\n",
            "Gen Loss: 0.7877064\n",
            "******************************\n",
            "Times: 16\n",
            "Dis Loss: 0.6419118642807007\n",
            "Gen Loss: 0.7667128\n",
            "******************************\n",
            "Times: 17\n",
            "Dis Loss: 0.6469337940216064\n",
            "Gen Loss: 0.72366655\n",
            "******************************\n",
            "Times: 18\n",
            "Dis Loss: 0.6322773694992065\n",
            "Gen Loss: 0.7768404\n",
            "******************************\n",
            "Times: 19\n",
            "Dis Loss: 0.6326279044151306\n",
            "Gen Loss: 0.82748353\n",
            "******************************\n",
            "Times: 20\n",
            "Dis Loss: 0.6116746664047241\n",
            "Gen Loss: 0.8107523\n",
            "******************************\n",
            "Times: 21\n",
            "Dis Loss: 0.6075621843338013\n",
            "Gen Loss: 0.8352881\n",
            "******************************\n",
            "Times: 22\n",
            "Dis Loss: 0.6179053783416748\n",
            "Gen Loss: 0.8725621\n",
            "******************************\n",
            "Times: 23\n",
            "Dis Loss: 0.6750074625015259\n",
            "Gen Loss: 0.82345563\n",
            "******************************\n",
            "Times: 24\n",
            "Dis Loss: 0.617378830909729\n",
            "Gen Loss: 0.89910287\n",
            "******************************\n",
            "Times: 25\n",
            "Dis Loss: 0.6371702551841736\n",
            "Gen Loss: 0.9725565\n",
            "******************************\n",
            "Times: 26\n",
            "Dis Loss: 0.6649667024612427\n",
            "Gen Loss: 0.96006256\n",
            "******************************\n",
            "Times: 27\n",
            "Dis Loss: 0.6286534070968628\n",
            "Gen Loss: 1.0452327\n",
            "******************************\n",
            "Times: 28\n",
            "Dis Loss: 0.6432418823242188\n",
            "Gen Loss: 1.0049521\n",
            "******************************\n",
            "Times: 29\n",
            "Dis Loss: 0.6571124792098999\n",
            "Gen Loss: 1.0644822\n",
            "******************************\n",
            "Times: 30\n",
            "Dis Loss: 0.6231870651245117\n",
            "Gen Loss: 0.97120345\n",
            "******************************\n",
            "Times: 31\n",
            "Dis Loss: 0.682397723197937\n",
            "Gen Loss: 0.9343612\n",
            "******************************\n",
            "Times: 32\n",
            "Dis Loss: 0.6709730625152588\n",
            "Gen Loss: 0.9044191\n",
            "******************************\n",
            "Times: 33\n",
            "Dis Loss: 0.6058405637741089\n",
            "Gen Loss: 0.93894136\n",
            "******************************\n",
            "Times: 34\n",
            "Dis Loss: 0.6316008567810059\n",
            "Gen Loss: 0.8390437\n",
            "******************************\n",
            "Times: 35\n",
            "Dis Loss: 0.687890887260437\n",
            "Gen Loss: 0.93344235\n",
            "******************************\n",
            "Times: 36\n",
            "Dis Loss: 0.6682259440422058\n",
            "Gen Loss: 0.868264\n",
            "******************************\n",
            "Times: 37\n",
            "Dis Loss: 0.648878812789917\n",
            "Gen Loss: 0.81807053\n",
            "******************************\n",
            "Times: 38\n",
            "Dis Loss: 0.681915283203125\n",
            "Gen Loss: 0.80658686\n",
            "******************************\n",
            "Times: 39\n",
            "Dis Loss: 0.6387364864349365\n",
            "Gen Loss: 0.7981796\n",
            "******************************\n",
            "Times: 40\n",
            "Dis Loss: 0.7030648589134216\n",
            "Gen Loss: 0.85518724\n",
            "******************************\n",
            "Times: 41\n",
            "Dis Loss: 0.6404939293861389\n",
            "Gen Loss: 0.8278871\n",
            "******************************\n",
            "Times: 42\n",
            "Dis Loss: 0.7008543014526367\n",
            "Gen Loss: 0.96378726\n",
            "******************************\n",
            "Times: 43\n",
            "Dis Loss: 0.5971767902374268\n",
            "Gen Loss: 1.0069788\n",
            "******************************\n",
            "Times: 44\n",
            "Dis Loss: 0.6405148506164551\n",
            "Gen Loss: 1.0055685\n",
            "******************************\n",
            "Times: 45\n",
            "Dis Loss: 0.6959646940231323\n",
            "Gen Loss: 1.1190897\n",
            "******************************\n",
            "Times: 46\n",
            "Dis Loss: 0.6365405917167664\n",
            "Gen Loss: 1.0784355\n",
            "******************************\n",
            "Times: 47\n",
            "Dis Loss: 0.6282951831817627\n",
            "Gen Loss: 1.0234438\n",
            "******************************\n",
            "Times: 48\n",
            "Dis Loss: 0.6026683449745178\n",
            "Gen Loss: 1.0593225\n",
            "******************************\n",
            "Times: 49\n",
            "Dis Loss: 0.5713273286819458\n",
            "Gen Loss: 1.0793899\n",
            "******************************\n",
            "Times: 50\n",
            "Dis Loss: 0.6195615530014038\n",
            "Gen Loss: 1.0606922\n",
            "******************************\n",
            "Times: 51\n",
            "Dis Loss: 0.6377252340316772\n",
            "Gen Loss: 1.0530876\n",
            "******************************\n",
            "Times: 52\n",
            "Dis Loss: 0.6337600946426392\n",
            "Gen Loss: 1.0020385\n",
            "******************************\n",
            "Times: 53\n",
            "Dis Loss: 0.6511304378509521\n",
            "Gen Loss: 0.9814267\n",
            "******************************\n",
            "Times: 54\n",
            "Dis Loss: 0.6917325258255005\n",
            "Gen Loss: 0.9887183\n",
            "******************************\n",
            "Times: 55\n",
            "Dis Loss: 0.6207332611083984\n",
            "Gen Loss: 1.0430602\n",
            "******************************\n",
            "Times: 56\n",
            "Dis Loss: 0.6334636211395264\n",
            "Gen Loss: 1.0108535\n",
            "******************************\n",
            "Times: 57\n",
            "Dis Loss: 0.6718524694442749\n",
            "Gen Loss: 1.0117868\n",
            "******************************\n",
            "Times: 58\n",
            "Dis Loss: 0.7574055194854736\n",
            "Gen Loss: 0.78266054\n",
            "******************************\n",
            "Times: 59\n",
            "Dis Loss: 0.7303017377853394\n",
            "Gen Loss: 0.8225362\n",
            "******************************\n",
            "Times: 60\n",
            "Dis Loss: 0.6811776757240295\n",
            "Gen Loss: 0.6793191\n",
            "******************************\n",
            "Times: 61\n",
            "Dis Loss: 0.7289335131645203\n",
            "Gen Loss: 0.76685876\n",
            "******************************\n",
            "Times: 62\n",
            "Dis Loss: 0.7531882524490356\n",
            "Gen Loss: 0.69494265\n",
            "******************************\n",
            "Times: 63\n",
            "Dis Loss: 0.7750109434127808\n",
            "Gen Loss: 0.7594879\n",
            "******************************\n",
            "Times: 64\n",
            "Dis Loss: 0.7121893167495728\n",
            "Gen Loss: 0.796299\n",
            "******************************\n",
            "Times: 65\n",
            "Dis Loss: 0.6932370662689209\n",
            "Gen Loss: 0.8578298\n",
            "******************************\n",
            "Times: 66\n",
            "Dis Loss: 0.5980925559997559\n",
            "Gen Loss: 1.0037147\n",
            "******************************\n",
            "Times: 67\n",
            "Dis Loss: 0.6253288984298706\n",
            "Gen Loss: 1.0026767\n",
            "******************************\n",
            "Times: 68\n",
            "Dis Loss: 0.5721051096916199\n",
            "Gen Loss: 1.3055072\n",
            "******************************\n",
            "Times: 69\n",
            "Dis Loss: 0.5595815181732178\n",
            "Gen Loss: 1.4122748\n",
            "******************************\n",
            "Times: 70\n",
            "Dis Loss: 0.5501190423965454\n",
            "Gen Loss: 1.2197245\n",
            "******************************\n",
            "Times: 71\n",
            "Dis Loss: 0.5976635813713074\n",
            "Gen Loss: 1.3142325\n",
            "******************************\n",
            "Times: 72\n",
            "Dis Loss: 0.58705735206604\n",
            "Gen Loss: 1.0771335\n",
            "******************************\n",
            "Times: 73\n",
            "Dis Loss: 0.594678521156311\n",
            "Gen Loss: 1.0529151\n",
            "******************************\n",
            "Times: 74\n",
            "Dis Loss: 0.6301014423370361\n",
            "Gen Loss: 1.1011682\n",
            "******************************\n",
            "Times: 75\n",
            "Dis Loss: 0.6646924018859863\n",
            "Gen Loss: 0.9882654\n",
            "******************************\n",
            "Times: 76\n",
            "Dis Loss: 0.6225570440292358\n",
            "Gen Loss: 0.90339553\n",
            "******************************\n",
            "Times: 77\n",
            "Dis Loss: 0.6642314195632935\n",
            "Gen Loss: 0.8817108\n",
            "******************************\n",
            "Times: 78\n",
            "Dis Loss: 0.6822270154953003\n",
            "Gen Loss: 0.8518829\n",
            "******************************\n",
            "Times: 79\n",
            "Dis Loss: 0.7278660535812378\n",
            "Gen Loss: 0.86030954\n",
            "******************************\n",
            "Times: 80\n",
            "Dis Loss: 0.7352938055992126\n",
            "Gen Loss: 0.7698864\n",
            "******************************\n",
            "Times: 81\n",
            "Dis Loss: 0.6897917985916138\n",
            "Gen Loss: 0.803188\n",
            "******************************\n",
            "Times: 82\n",
            "Dis Loss: 0.6718531847000122\n",
            "Gen Loss: 0.8370865\n",
            "******************************\n",
            "Times: 83\n",
            "Dis Loss: 0.6456030607223511\n",
            "Gen Loss: 0.8329898\n",
            "******************************\n",
            "Times: 84\n",
            "Dis Loss: 0.5985279679298401\n",
            "Gen Loss: 1.002127\n",
            "******************************\n",
            "Times: 85\n",
            "Dis Loss: 0.5862279534339905\n",
            "Gen Loss: 0.94617355\n",
            "******************************\n",
            "Times: 86\n",
            "Dis Loss: 0.6139690279960632\n",
            "Gen Loss: 1.1433867\n",
            "******************************\n",
            "Times: 87\n",
            "Dis Loss: 0.6140187382698059\n",
            "Gen Loss: 0.9813267\n",
            "******************************\n",
            "Times: 88\n",
            "Dis Loss: 0.6686669588088989\n",
            "Gen Loss: 0.9422148\n",
            "******************************\n",
            "Times: 89\n",
            "Dis Loss: 0.6430876851081848\n",
            "Gen Loss: 0.9606308\n",
            "******************************\n",
            "Times: 90\n",
            "Dis Loss: 0.6866834163665771\n",
            "Gen Loss: 0.8279381\n",
            "******************************\n",
            "Times: 91\n",
            "Dis Loss: 0.6335558295249939\n",
            "Gen Loss: 0.79752594\n",
            "******************************\n",
            "Times: 92\n",
            "Dis Loss: 0.6148698329925537\n",
            "Gen Loss: 0.75034136\n",
            "******************************\n",
            "Times: 93\n",
            "Dis Loss: 0.6868311166763306\n",
            "Gen Loss: 0.790314\n",
            "******************************\n",
            "Times: 94\n",
            "Dis Loss: 0.6170778274536133\n",
            "Gen Loss: 0.75944704\n",
            "******************************\n",
            "Times: 95\n",
            "Dis Loss: 0.6607991456985474\n",
            "Gen Loss: 0.8069828\n",
            "******************************\n",
            "Times: 96\n",
            "Dis Loss: 0.6603526473045349\n",
            "Gen Loss: 0.82853746\n",
            "******************************\n",
            "Times: 97\n",
            "Dis Loss: 0.6622956991195679\n",
            "Gen Loss: 0.9177921\n",
            "******************************\n",
            "Times: 98\n",
            "Dis Loss: 0.6956030130386353\n",
            "Gen Loss: 0.8219507\n",
            "******************************\n",
            "Times: 99\n",
            "Dis Loss: 0.6881110668182373\n",
            "Gen Loss: 0.85818774\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-coUWLpp4ZVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "5f86ee98-0226-4c10-85c7-fc14b148e878"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# 拿出5個examples\n",
        "examples = 5\n",
        "# 準備五個靈感\n",
        "noise = np.random.normal(0, 1, (examples, 100))\n",
        "# 使用創作者開始創作\n",
        "gen_imgs = generator.predict(noise)\n",
        "\n",
        "# 這裡要注意一下, 必須讓-1-1回到0-1才能被正確印出來\n",
        "gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "# reshape成為圖片\n",
        "gen_imgs = gen_imgs.reshape(examples, 28, 28)\n",
        "# 設定一下完整畫布大小\n",
        "plt.figure(figsize = (14, 14))\n",
        "for i in range(0, examples):\n",
        "    # 將大圖分成1 * 5五小圖, 編號分別為\n",
        "    # 1, 2, 3, 4, 5\n",
        "    # 所以i必須+1來得到相對應的小圖\n",
        "    plt.subplot(1, examples, i + 1)\n",
        "    # 不打印座標軸\n",
        "    plt.axis('off')\n",
        "    # 秀出圖片\n",
        "    plt.imshow(gen_imgs[i], cmap='gray')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACVCAYAAAAwjOf8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1008x1008 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}